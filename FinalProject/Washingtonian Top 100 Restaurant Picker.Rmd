---
title: "Washingtonian Top 100 Restaurant Picker"
author: "Sungjoo Cho and Kate Lamoreaux"
date: "2023-10-28"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Libraries

#Feedback from Brian: How do we optimize restaurant choices? Building an interactive tool. MAke sure you have a clear goal in mind. RQ = can be goal we have in mind.

Ideas: could link `$$$$` ratings/star ratings/mapdata/website from google, star ratings from yelp (use yelp api; scraping is illegal).

```{r message=FALSE, warning=FALSE}
library(xml2)
library(rvest)
library(tidyverse)
library(tidytext)
library(jsonlite)
library(robotstxt)
library(RSocrata)
library(googleway)
library(httr)
library(readxl)
```


## 1. Washingtonian Web Scraping

```{r}
# see whether I can scrape
paths_allowed("https://www.washingtonian.com/2023/01/25/the-100-very-best-restaurants-in-washington-dc/")
```

Since this is true, I can scrape!

```{r}
# Reading in the html page as R object url
url <- read_html("https://www.washingtonian.com/2023/01/25/the-100-very-best-restaurants-in-washington-dc/")
```

```{r}
# Extracting the nodes from object url (using the `rvest` package) 
nds <- html_elements(url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "sm-tw-py-0", " " ))]')

str(nds)
```

```{r}
names <- html_text(nds)
head(names)
```

```{r}
#convert to a tibble
tophundred <- as_tibble(names)
head(tophundred)
```

```{r}
tophundred$value[1]
```

```{r}
# Remove "\n"
library(stringr)
tophundred$value <- gsub("[\n]+", "sepcol", tophundred$value)
tophundred$value[1]
```


```{r}
# Remove "sepcolRead Our ReviewsepcolVisit Website sepcol"
tophundred$value <- gsub("sepcolRead Our ReviewsepcolVisit Website sepcol", "", tophundred$value)
head(tophundred)
```

```{r}
#Using separate to clean and label columns
tophundred_clean <- tophundred %>%
  separate(value,c("blank","ranking","name", "ranking2", "name2", "genre", "address", "phone number"), 'sepcol') 
head(tophundred_clean)
```


```{r}
tophundred_final <- tophundred_clean %>%
  select(-c("blank", "ranking2", "name2"))
head(tophundred_final)
```

```{r}
# # exporting the data
# install.packages("openxlsx")
# library(openxlsx)
# exportlist <- write.xlsx(tophundred_final, "/Users/sungjoocho/Desktop/UMD/Fall2023/SURV727/Group_assignment/cho-lamoreaux-a1/wasingtonian_top100.xlsx")
```


## 2. Yelp API Data
```{r}
# Load Data from Yelp (obtained using Python)
yelp <- read.csv("yelp.csv")
yelp <- as.tibble(yelp)
```

## 3. Google Places API Data
```{r}

```


## 4. Merging Washingtonion Web Scraping Data and Yelp API Data

We found discrepancies in restaurant names between Washingtonian and Yelp data, such as 'Pineapple and Pearls' in Washingtonian versus 'Pineapple & Pearls' in Yelp. To address this, we created a new variable called 'key_name' for merging these datasets. This variable eliminates all special characters and spaces to ensure accurate matching.

```{r}
# create key_name variable for merging in tophundred_final dataset 
tophundred_final$key_name <- gsub("[^A-Za-z0-9]", "", tophundred_final$name)
head(tophundred_final)

# creating key_name variable for merging in yelp dataset
yelp$key_name <- gsub("&", "and", yelp$name) # change & to "and"
yelp$key_name <- gsub("[^A-Za-z0-9]", "", yelp$key_name)
head(yelp)
```

```{r}
# merge yelp data to washingtonion
washington_yelp <- tophundred_final %>% left_join(yelp, by = "key_name")
head(washington_yelp)
```

## 4. Data Cleaning

```{r}
# remove key_name variables
washington_yelp <- washington_yelp[, c("ranking", "name.x", "genre", "categories", "address.x", "address.y", "phone number", "phone", "rating", "price", "latitude", "longitude")]

# export data for manual cleaning
library(openxlsx)
write.xlsx(washington_yelp, "/Users/sungjoocho/Desktop/UMD/Fall2023/SURV727/Group_assignment/cho-lamoreaux-a1/FinalProject/washington_yelp.xlsx")

# load manually cleaned data
library(readxl)
data <- read_excel("washington_yelp.xlsx")
head(data)
```






