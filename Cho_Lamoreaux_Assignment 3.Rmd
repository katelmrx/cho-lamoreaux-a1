---
title: "Assignment 3"
author: "Sungjoo Cho, Catherine (Kate) Lamoreaux"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Our GitHub Repository

<https://github.com/katelmrx/cho-lamoreaux-a1>

### Packages

```{r, message=FALSE}
library(xml2)
library(rvest)
library(tidyverse)
library(robotstxt)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(stringi)
```

## Web Scraping

## Wikipedia

```{r}
# Review whether we are allowed to do web scraping
paths_allowed("https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago")
```
Because the result is true, we are allowed to scrape from Wikipedia.

```{r}
# # Reading in the html page as R object url
url_gb <- read_html("https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago")
str(url_gb)
```

## 1. Grab the html elements

As a first step, read in the html page as an R object. Extract the tables from this object (using the `rvest` package) and save the result as a new object. Follow the instructions if there is an error. Use `str()` on this new object. It should be a list. Try to find the position of the "Historical population" in this list since we need it in the next step.

```{r}
# Extracting the tables from object url_gb (using the `rvest` package) 
nds <- html_elements(url_gb, xpath = '//table')
```

```{r}
# Using `str()` on new object nds, which is a list
str(nds)
```

```{r}
# Using html_table() on the nds object to search for the 
# "Historical population" table within this list 
table_details <- html_table(nds)
head(table_details)
```

The "Historical population" table is in [[2]] of the table_details list.

Extract the "Historical population" table from the list and save it as another object. You can use subsetting via `[[â€¦]]` to extract pieces from a list. Print the result.

```{r}
# Extracting the "Historical population" table from list table_details 
# Saving this as object pop
pop <- table_details[[2]]

# Printing pop
print(pop)
```

```{r}
# Keeping only rows and columns with actual values
pop <- pop[1:10, -3]
pop
```

## 2. Expanding to More Pages

That's it for this page. However, we may want to repeat this process for other community areas. The Wikipedia page [https://en.wikipedia.org/wiki/Grand_Boulevard,\\\_Chicago](https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago){.uri} has a section on "Places adjacent to Grand Boulevard, Chicago" at the bottom. Can you find the corresponding table in the list of tables that you created earlier? Extract this table as a new object.

The "Places adjacent to Grand Boulevard, Chicago" table is in [4] of the table_details list.

```{r}
# Extracting "Places adjacent to Grand Boulevard, Chicago" table
# Assigning this table to object gb_adj
(gb_adj <- table_details[[4]])
```

Then, grab the community areas east of Grand Boulevard and save them as a character vector. Print the result.

```{r}
# Saving communities east of Grand Boulevard and removing blank characters from
# the character string, named east_gb
(east_gb <- gb_adj$X3 %>% stri_remove_empty_na())
```

We want to use this list to create a loop that extracts the population tables from the Wikipedia pages of these places. To make this work and build valid urls, we need to replace empty spaces in the character vector with underscores. This can be done with `gsub()`, or by hand. The resulting vector should look like this: "Oakland,\_Chicago" "Kenwood,\_Chicago" "Hyde_Park,\_Chicago"

```{r}
# Using gsub to replace the empty spaces of the character vector east_gb
# with underscores
(east_gb <- gsub(" ", "_", east_gb))
```

To prepare the loop, we also want to copy our `pop` table and rename it as `pops`. In the loop, we append this table by adding columns from the other community areas.

```{r}
pops <- pop
```

Build a small loop to test whether you can build valid urls using the vector of places and pasting each element of it after `https://en.wikipedia.org/wiki/` in a for loop. Calling `url` shows the last url of this loop, which should be `https://en.wikipedia.org/wiki/Hyde_Park,_Chicago`.

```{r}
# Building a small test loop
for(i in east_gb) {
   url <- paste0("https://en.wikipedia.org/wiki/",i, sep = "")
   src <- read_html(url)
  print(url)   }

url
```

Finally, extend the loop and add the code that is needed to grab the population tables from each page. Add columns to the original table `pops` using `cbind()`.

```{r}
# Extending the loop
for(i in east_gb) {
   url <- paste0("https://en.wikipedia.org/wiki/",i)
   src <- read_html(url)
  print(url)   

# Extracting the tables from object src (using the `rvest` package) 
    nds <- html_elements(src, xpath = '//table')
  
# Using html_table() on the nds object  
    table_details <- html_table(nds)

# Extracting the "Historical population" table from list table_details 
# Saving this as object pop
  pop <- table_details[[2]]

# Keeping only rows and columns with actual values
  pop <- pop[1:10, -3]

pops <- cbind(pops, pop)
  
}

pops
str(pops)
```

## 3. Scraping and Analyzing Text Data

Suppose we wanted to take the actual text from the Wikipedia pages instead of just the information in the table. Our goal in this section is to extract the text from the body of the pages, then do some basic text cleaning and analysis.

First, scrape just the text without any of the information in the margins or headers. For example, for "Grand Boulevard", the text should start with, "**Grand Boulevard** on the [South Side](https://en.wikipedia.org/wiki/South_Side,_Chicago "South Side, Chicago") of [Chicago](https://en.wikipedia.org/wiki/Chicago "Chicago"), [Illinois](https://en.wikipedia.org/wiki/Illinois "Illinois"), is one of the ...".

### (1) Creating a corpus

```{r}
# Extracting desired text elements from webpage
nds_text <- html_elements(url_gb, xpath = "//p")
str(nds_text)

# Reading html text
(description <- html_text(nds_text))

# Making sure all of the text is in one block
(description <- description %>% paste(collapse = ' '))
```

### (2) Grab the descriptions of the various communities areas using for loop

Using a similar loop as in the last section, grab the descriptions of the various communities areas. Make a tibble with two columns: the name of the location and the text describing the location.

```{r}
# Making a copy of description to prepare the loop
descriptions <- description
```

```{r}
# Using a similar loop to grab descriptions of the eastern community areas
for(i in east_gb) {
   url <- paste0("https://en.wikipedia.org/wiki/",i)
   src <- read_html(url)
  print(url)   

 # Text source
  nds_text_src <- html_elements(src, xpath = "//p")

# Extracting the tables from object nds_text_src 
description <- html_text(nds_text_src)

# Make sure all of the text is in one block
description <- description %>% paste(collapse = ' ')

descriptions <- rbind(descriptions, description)
}

# Viewing output
str(descriptions)
```

```{r}
# Making a tibble with two columns: the name of the location and 
# the text describing the location.
descriptions_df <- as.tibble(descriptions)

chi_neighborhood <- data.frame(
  "neighborhood" = c("Grand Boulevard","Oakland", "Kenwood","Hyde Park"),
  "description_text" = descriptions_df$V1)

chi_neighborhood <- as.tibble(chi_neighborhood)
head(chi_neighborhood)
```

### (3) Create tokens using unnest tokens. Make sure the data is in one token per row format. Remove any stop words within the data. What are the most common words used overall?

```{r}
# Creating tokens using unnest_tokens
chi_tokens <- chi_neighborhood %>%
  unnest_tokens(word, description_text)

# Making sure the data is in one token per row format
head(chi_tokens)
```

```{r}
# Removing stop words
data("stop_words")
chi_tokens <- chi_tokens %>%
  anti_join(stop_words)
head(chi_tokens)
```

```{r}
# Count the most common words used overall
token_count <- chi_tokens %>%
  count(word, sort = TRUE)
head(token_count)

# Plot the most common words used overall
token_count %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(x = "Frequency Used", y = NULL)
```

Upon removing stop words, the most common words used overall are: "park" (n=89), "hyde" (n=75), and "chicago" (n=57).

### (4) Plot the most common words within each location. What are some of the similarities between the locations? What are some of the differences?

```{r}
(grouped_tokens <- chi_tokens %>%
  group_by(neighborhood)%>%
  count(word, sort = TRUE))
```

```{r}
# plot for the most common words within Grand Boulevard
token_Grand_Boulevard <- grouped_tokens %>%
  filter(neighborhood == "Grand Boulevard") %>%
  slice(1:6) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill="gold") +
  labs(title = "Grand Boulevard Description", x = "Frequency Used", y = NULL)

# plot for the most common words within Oakland
token_Oakland <- grouped_tokens %>%
  filter(neighborhood == "Oakland") %>%
  slice(1:6) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill="orchid") +
  labs(title = "Oakland Description", x = "Frequency Used", y = NULL)

# plot for the most common words within Kenwood
token_Kenwood <- grouped_tokens %>%
  filter(neighborhood == "Kenwood") %>%
  slice(1:6) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill="royalblue") +
  labs(title = "Kenwood Description", x = "Frequency Used", y = NULL)

# plot for the most common words within Hyde_Park
token_Hyde_Park <- grouped_tokens %>%
  filter(neighborhood == "Hyde Park") %>%
  slice(1:6) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill="seagreen") +
  labs(title = "Hyde Park Description", x = "Frequency Used", y = NULL)
```

```{r}
# plots
token_Grand_Boulevard
token_Oakland
token_Kenwood
token_Hyde_Park
```

The most common words used in the Grand Boulevard description are: "boulevard", "grand", and "chicago".

The most common words used in the Oakland description are: "oakland", "chicago", and "housing".

The most common words used in the Kenwood description are: "kenwood", "school", and "park".

The most common words used in the Hyde Park description are: "park", "hyde", and "chicago".

All of the neighborhood descriptions had parts of each respective neighborhood name as some of their top used words. Unsurprisingly, "chicago" was in the top 6 words used in all of the neighborhoods. Neighborhoods also featured words that reflected what was in them. For instance the Hyde Park neighborhood description had "university" as its fifth most frequently used word, which makes sense as Hyde Park is home to the University of Chicago.

Hyde Park in contrast to the other neighborhoods, had much higher frequencies of words used, with all six most frequently used words having usages above 10. All the other neighborhoods only had their top word used at or over 10 times. This is likely because the Hyde Park article has the longest description out of these four neighborhoods.
